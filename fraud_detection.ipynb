{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have trained total of 6 different models and evaluated their performance on the fraud detection problem. As the data was not balanced, at first I trained three different models: Logistic Regression, Random Forest Classifier and XGBoost Classifier on the imbalanced data then later those models on oversampled data. The hyperparameters were tuned separately so in total there are 6 different models. \n",
    "\n",
    "The functions used in this notebook are declared in utils.py. \n",
    "\n",
    "Steps included in this notebook:\n",
    "\n",
    "* Import library\n",
    "* Read and load data\n",
    "* Exploratory Data Analysis (EDA)\n",
    "* Visualization of data\n",
    "* Feature Selection\n",
    "* Split of data\n",
    "* Hyperparameter Tuning \n",
    "* Model Fitting\n",
    "* Model Evaluation\n",
    "* Oversampling data\n",
    "* Hyperparameter Tuning \n",
    "* Model Fitting\n",
    "* Model Evaluation\n",
    "* Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impoting necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "data_path = 'creditcard_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = u.load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing column names\n",
    "print(\"\\nColumns:\\n\",df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check type of data in the dataframe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing null values count\n",
    "print(\"\\nNumber of null values in each column:\\n\",df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing statistical info\n",
    "print(\"\\nStatistics of all the columns of the dataframe: \\n\",df.describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values count for each column\n",
    "print(\"\\nUnique values count in each column: \\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print skewness of each column\n",
    "print(\"\\nSkewness for each column: \\n\", df.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print kurtosis of each column\n",
    "print(\"\\nKurtosis for each column: \\n\", df.kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and print if there are any duplicate rows\n",
    "print(\"\\nNumber of duplicate rows: \\n\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numeric columns, create box plots to check for outliers\n",
    "u.visualize_boxplots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main variables\n",
    "target_var = 'Class'\n",
    "amount_column = 'Amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Class Difference\n",
    "u.class_difference(df, target_var, amount_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Class Difference\n",
    "u.visualize_class_difference(df, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Heatmap\n",
    "u.visualize_heatmap(df, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Scatterplot\n",
    "u.visualize_scatterplot(df, target_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "features_selected = u.feature_selection(df, target_var)\n",
    "\n",
    "# Concatenate selected features with 'Class' and 'Amount' columns\n",
    "df_final = pd.concat([df[features_selected], df[['Class', 'Amount']]], axis=1)\n",
    "print(\"\\nSelected features: \", df_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = u.split_dataset(df_final, target_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, three different models: Logistic Regression, Random Forest Classifier and XGB CLassifier are trained of the data without balancing it. The hyperparameters are tuned before training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_lr = u.hyperparameter_tuning_logistic(X_train, y_train)\n",
    "\n",
    "# Create a new LogisticRegression with the best hyperparameters\n",
    "best_lr_model = LogisticRegression(**best_params_lr)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained LogisticRegression model\n",
    "lr_pred = best_lr_model.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, lr_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, lr_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_rf = u.hyperparameter_tuning_rf(X_train, y_train)\n",
    "\n",
    "# Create a new RandomForestClassifier with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(**best_params_rf)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained RandomForestClassifier model\n",
    "rf_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, rf_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_xgb = u.hyperparameter_tuning_xgb(X_train, y_train)\n",
    "\n",
    "# Create a new XGBClassifier with the best hyperparameters\n",
    "best_xgb_model = XGBClassifier(**best_params_xgb)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained XGBClassifier model\n",
    "xgb_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, xgb_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is oversampled using SMOTE and again the same three models are trained by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample\n",
    "X_train_oversampled, y_train_oversampled = u.oversample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression on Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_lr_os = u.hyperparameter_tuning_logistic(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Create a new LogisticRegression with the best hyperparameters\n",
    "best_lr_model_os = LogisticRegression(**best_params_lr_os)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_lr_model_os.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained LogisticRegression model\n",
    "lr_os_pred = best_lr_model_os.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, lr_os_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, lr_os_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier on Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_rf_os = u.hyperparameter_tuning_rf(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Create a new RandomForestClassifier with the best hyperparameters\n",
    "best_rf_model_os = RandomForestClassifier(**best_params_rf_os)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_rf_model_os.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained RandomForestClassifier model\n",
    "rf_os_pred = best_rf_model_os.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, rf_os_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, rf_os_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB Classifier on Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_xgb_os = u.hyperparameter_tuning_xgb(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Create a new XGBClassifier with the best hyperparameters\n",
    "best_xgb_model_os = XGBClassifier(**best_params_xgb_os)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_xgb_model_os.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Performance of XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the X_test using the trained XGBClassifier model\n",
    "xgb_os_pred = best_xgb_model_os.predict(X_test)\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "u.print_evaluation_metrics(y_test, xgb_os_pred)\n",
    "\n",
    "# Grpah the confusion matrix of the predicted result\n",
    "u.plot_confusion_matrix(y_test, xgb_os_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "The results of all the models with and without oversampling seemed to be similar for this data.\n",
    "\n",
    "Possible improvemets in the process:\n",
    "\n",
    "* Remove outliers and compare the result. As it is a fraud detection problem I didnot remove the outliers because data that seems to be the outliers could be the actual positive cases in these kind of problems. And removing them will increase class imbalance.\n",
    "\n",
    "* Try different feature selection approach and compare the results.\n",
    "\n",
    "* Different approaches to balance the data. Instead of just using SMOTE to oversample the minority, undersampling or class weight or sample weight can be used as they might perform better in this case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
